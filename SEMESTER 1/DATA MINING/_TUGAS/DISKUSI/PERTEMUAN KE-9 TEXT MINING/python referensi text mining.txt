
ijin menjawab pak , di sini saya mengunakan data sheet heart disease dan berikut hasil implementasi text mining menggunakan algoritma Naive Bayes pada program python
##########
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, log_loss, matthews_corrcoef, roc_auc_score, roc_curve
from sklearn.naive_bayes import GaussianNB
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Load data from Excel
data = pd.read_excel("D:/doc/data mining/Program/data/heart_disease.xlsx")

# Gabungkan kolom-kolom kategori menjadi satu kolom teks
categorical_columns = ['gender', 'thal', 'rest ECG', 'slope peak exc ST', 'chest pain']
data['combined_text'] = data[categorical_columns].astype(str).agg(' '.join, axis=1)

# Preprocessing text data
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
words = word_tokenize(text)
words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]
words = [lemmatizer.lemmatize(word) for word in words]
return ' '.join(words)

# Apply preprocessing to the combined text column
data['processed_text'] = data['combined_text'].apply(preprocess)

# Vectorize text data using TF-IDF
tfidf_vectorizer = TfidfVectorizer()
X_tfidf = tfidf_vectorizer.fit_transform(data['processed_text'])

# Split features and target variable
X = X_tfidf
y = data['thal'] # Ganti 'target_column' dengan nama kolom target Anda

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Initialize Naive Bayes classifier
clf = GaussianNB()

# Train and evaluate the model
# Convert sparse matrix to dense since GaussianNB does not support sparse matrix
X_train_dense = X_train.toarray()
X_test_dense = X_test.toarray()

clf.fit(X_train_dense, y_train)
y_pred = clf.predict(X_test_dense)
y_proba = clf.predict_proba(X_test_dense)

# Calculate metrics
cm = confusion_matrix(y_test, y_pred)
ca = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')
precision = precision_score(y_test, y_pred, average='weighted', zero_division=1) # Handle division by zero case
recall = recall_score(y_test, y_pred, average='weighted')
logloss = log_loss(y_test, y_proba)
mcc = matthews_corrcoef(y_test, y_pred)
auroc = roc_auc_score(y_test, y_proba, multi_class='ovr')

# Print metrics
print("Naive Bayes:")
print("Confusion Matrix:")
print(cm)
print("Accuracy:", ca)
print("F1 Score:", f1)
print("Precision:", precision)
print("Recall:", recall)
print("Log Loss:", logloss)
print("MCC:", mcc)
print("AUROC:", auroc)

# Plot AUROC Curve for each class
plt.figure()
for i in range(len(clf.classes_)):
if np.sum(y_test == clf.classes_[i]) == 0:
print(f"Skipping class {clf.classes_[i]} due to no positive samples in y_test.")
continue
fpr, tpr, _ = roc_curve(y_test, y_proba[:, i], pos_label=clf.classes_[i])
plt.plot(fpr, tpr, lw=2, label='Class %s (area = %0.2f)' % (str(clf.classes_[i]), roc_auc_score(y_test == clf.classes_[i], y_proba[:, i])))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Naive Bayes')
plt.legend(loc="lower right")
plt.show()
#########